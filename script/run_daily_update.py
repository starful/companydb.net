import pandas as pd
import os
import frontmatter
import google.generativeai as genai
from dotenv import load_dotenv
import time
import re
import json
from datetime import datetime

# --- ì„¤ì •: ìƒìˆ˜ ëª¨ìŒ ---
load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

MODEL = genai.GenerativeModel('gemini-1.5-flash')
DAILY_LIMIT = 10

# â–¼â–¼â–¼ [ìˆ˜ì •ë¨] ì¼ê´€ëœ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì •ì˜ â–¼â–¼â–¼
CATEGORIES = ["Manufacturing", "Technology", "Electronics", "Medical", "Construction", "Services"]

CSV_PATH = 'data/Total_Premium_Japan_SMEs.csv'
CONTENT_DIR = 'app/content'
DATA_DIR = 'data'
INDEX_PATH = os.path.join(DATA_DIR, 'search_index.json')
SITEMAP_PATH = 'app/static/sitemap.xml'
DOMAIN = "https://companydb.net"

def slugify(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]+', '-', text)
    return text.strip('-')

def generate_new_content():
    if not os.path.exists(CSV_PATH):
        print(f"âŒ ì›ë³¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {CSV_PATH}")
        return

    df = pd.read_csv(CSV_PATH)
    os.makedirs(CONTENT_DIR, exist_ok=True)
    
    count = 0
    print(f"ğŸš€ ì´ {len(df)}ê°œ ê¸°ì—… ì¤‘ ìµœëŒ€ {DAILY_LIMIT}ê°œì˜ ì‹ ê·œ ë¦¬í¬íŠ¸ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    for _, row in df.iterrows():
        if count >= DAILY_LIMIT:
            print(f" ì¼ì¼ ìƒì„± ì œí•œ({DAILY_LIMIT}ê°œ)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
            break
        
        cid = f"jp_{row['corporate_number']}"
        if any(f.startswith(cid) for f in os.listdir(CONTENT_DIR)):
            continue

        print(f"   [ {count+1} / {DAILY_LIMIT} ] '{row['name']}' ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # â–¼â–¼â–¼ [ìˆ˜ì •ë¨] ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ë¥¼ ìš”ì²­í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ê°•í™” â–¼â–¼â–¼
        prompt = f"""
        Act as a Senior Business Analyst. Analyze the following company and provide a B2B report.

        - Company: {row['name']}
        - Location: {row['location']}
        - Gov Info: {row['subsidy_titles'] if pd.notna(row['subsidy_titles']) else "Verified SME"}

        [Output Instructions]
        Line 1: A formal English name. If none is found, REPEAT the original Japanese name.
        Line 2: Choose the ONE most appropriate category from this list: {', '.join(CATEGORIES)}. Output in the format "Category: [Chosen Category]".
        Line 3: ---BODY---
        Line 4 and beyond: Full Detailed Markdown Report (Min 4,000 chars).

        [Analyst's Note Instructions]
        - At the VERY BEGINNING of the markdown body, create a short, insightful "Analyst's Note" blockquote.
        - This note should summarize the company's core B2B value proposition.

        [Content Focus & Formatting Rules]
        - Professional B2B perspective. Be verbose.
        - Use standard Markdown: headings (##), lists (*), etc.
        """
        # â–²â–²â–² [ìˆ˜ì •ë¨] ë â–²â–²â–²
        
        try:
            response = MODEL.generate_content(prompt)
            full_response = response.text.strip()

            header_part, ai_content = full_response.split("---BODY---", 1)
            header_lines = header_part.strip().split('\n')

            ai_en_name = header_lines[0].strip()
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ë° ê¸°ë³¸ê°’ ì„¤ì •
            ai_category = "Services" # ê¸°ë³¸ê°’
            if len(header_lines) > 1 and "Category:" in header_lines[1]:
                ai_category = header_lines[1].replace("Category:", "").strip()
                # AIê°€ ëª©ë¡ì— ì—†ëŠ” ê°’ì„ ìƒì„±í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì•ˆì „ì¥ì¹˜
                if ai_category not in CATEGORIES:
                    ai_category = "Services"

            if not ai_en_name.isascii():
                ai_en_name = str(row['name'])

            file_slug = slugify(ai_en_name)
            
            if file_slug:
                file_name = f"{cid}_{file_slug}.md"
            else:
                file_name = f"{cid}.md"
            
            file_path = os.path.join(CONTENT_DIR, file_name)

            metadata = {
                "id": cid,
                "title": str(row['name']),
                "title_en": ai_en_name,
                "address": str(row['location']),
                "subsidies": int(row['subsidy_count']),
                "category": ai_category, # AIê°€ ë¶„ë¥˜í•œ ì¹´í…Œê³ ë¦¬ ì €ì¥
                "contact": f"https://www.google.com/search?q={row['name']}+contact+website"
            }
            
            post = frontmatter.Post(ai_content.strip(), **metadata)
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(frontmatter.dumps(post))
            
            print(f"   âœ… ì™„ë£Œ: {file_name} (Category: {ai_category})")
            count += 1
            time.sleep(5)
            
        except Exception as e:
            print(f"   âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            time.sleep(10)
    
    if count == 0:
        print("ìƒì„±í•  ìƒˆë¡œìš´ ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")

def update_index_and_sitemap():
    if not os.path.exists(CONTENT_DIR):
        print(f"âš ï¸ ì½˜í…ì¸  í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {CONTENT_DIR}")
        return
    
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs('app/static', exist_ok=True)

    index_data = []
    sitemap_urls = []
    
    print(f"\nğŸ” '{CONTENT_DIR}' í´ë”ë¥¼ ìŠ¤ìº”í•˜ì—¬ ì¸ë±ì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    for filename in os.listdir(CONTENT_DIR):
        if filename.endswith('.md'):
            try:
                file_path = os.path.join(CONTENT_DIR, filename)
                with open(file_path, 'r', encoding='utf-8') as f:
                    post = frontmatter.load(f)
                    file_slug = filename.replace(".md", "")
                    
                    # â–¼â–¼â–¼ [ìˆ˜ì •ë¨] ê²€ìƒ‰ ì¸ë±ìŠ¤ì— ì¹´í…Œê³ ë¦¬('c') í•„ë“œ ì¶”ê°€ â–¼â–¼â–¼
                    index_data.append({
                        "id": post.metadata.get('id', ''),
                        "file": file_slug,
                        "n": post.metadata.get('title', ''),
                        "en": post.metadata.get('title_en', ''),
                        "l": str(post.metadata.get('address', ''))[:30],
                        "s": post.metadata.get('subsidies', 0),
                        "c": post.metadata.get('category', 'Services') # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€
                    })
                    # â–²â–²â–² [ìˆ˜ì •ë¨] ë â–²â–²â–²

                    mtime = os.path.getmtime(file_path)
                    lastmod = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d')
                    
                    sitemap_urls.append(f"""
    <url>
        <loc>{DOMAIN}/company/{file_slug}</loc>
        <lastmod>{lastmod}</lastmod>
        <changefreq>weekly</changefreq>
        <priority>0.8</priority>
    </url>""")
            except Exception as e:
                print(f"âš ï¸ {filename} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
    
    with open(INDEX_PATH, 'w', encoding='utf-8') as f:
        json.dump(index_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… ê²€ìƒ‰ ì¸ë±ìŠ¤ ê°±ì‹  ì™„ë£Œ ({len(index_data)}ê°œ ê¸°ì—…)")

    sitemap_content = f"""<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
    <url>
        <loc>{DOMAIN}/</loc>
        <changefreq>daily</changefreq>
        <priority>1.0</priority>
    </url>
    {''.join(sitemap_urls)}
</urlset>"""
    with open(SITEMAP_PATH, 'w', encoding='utf-8') as f: f.write(sitemap_content)
    print(f"âœ… ì‚¬ì´íŠ¸ë§µ ìƒì„± ì™„ë£Œ: {SITEMAP_PATH} ({len(sitemap_urls)}ê°œ ë§í¬)")

def main():
    print("="*40)
    print("  CompanyDB ì¼ì¼ ì—…ë°ì´íŠ¸ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ")
    print("="*40)
    generate_new_content()
    update_index_and_sitemap()
    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("="*40)

if __name__ == "__main__":
    main()