import pandas as pd
import os
import frontmatter
import google.generativeai as genai
from dotenv import load_dotenv
import time
import re

# ì„¤ì • ë¡œë“œ
load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
model = genai.GenerativeModel('gemini-2.0-flash')

CSV_PATH = 'data/Total_Premium_Japan_SMEs.csv'
CONTENT_DIR = 'app/content'
DAILY_LIMIT = 10

def slugify(text):
    """ì˜ì–´ íšŒì‚¬ëª…ì„ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜"""
    text = text.lower()
    text = re.sub(r'[^a-z0-9]+', '-', text)
    return text.strip('-')

def generate_md():
    df = pd.read_csv(CSV_PATH)
    os.makedirs(CONTENT_DIR, exist_ok=True)
    
    count = 0
    print(f"ğŸš€ ì¥ë¬¸ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘ (JSON ë¯¸ì‚¬ìš©, í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)")

    for _, row in df.iterrows():
        if count >= DAILY_LIMIT: break
        
        cid = f"jp_{row['corporate_number']}"
        existing_files = [f for f in os.listdir(CONTENT_DIR) if f.startswith(cid)]
        if existing_files: continue

        print(f"ğŸ“ [{count+1}/{DAILY_LIMIT}] {row['name']} ë¶„ì„ ì¤‘...")
        
        # AIì—ê²Œ JSON ëŒ€ì‹  êµ¬ì¡°í™”ëœ ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ ìš”êµ¬
        prompt = f"""
        Act as a Senior Business Analyst. Write a 4,000+ character B2B analysis report for:
        - Company: {row['name']}
        - Location: {row['location']}
        - Gov Info: {row['subsidy_titles'] if pd.notna(row['subsidy_titles']) else "Verified SME"}

        [Output Instructions]
        Line 1: Formal English Name (Only the name, nothing else)
        Line 2: ---BODY---
        Line 3 and beyond: Full Detailed Markdown Report (Min 4,000 chars)

        [Content Focus]
        - Professional B2B perspective.
        - Analyze Industry Context, Monozukuri/Quality, Regional Advantage.
        - Be extremely verbose to exceed 4,000 characters.
        """
        
        try:
            response = model.generate_content(prompt)
            full_response = response.text.strip()

            # êµ¬ë¶„ì(---BODY---)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ë¦„ê³¼ ë³¸ë¬¸ì„ ë‚˜ëˆ”
            if "---BODY---" in full_response:
                parts = full_response.split("---BODY---")
                ai_en_name = parts[0].strip()
                ai_content = parts[1].strip()
            else:
                # êµ¬ë¶„ìê°€ ì—†ì„ ê²½ìš° ì²« ì¤„ì„ ì´ë¦„ìœ¼ë¡œ ê°„ì£¼
                lines = full_response.split('\n')
                ai_en_name = lines[0].strip()
                ai_content = "\n".join(lines[1:]).strip()

            # íŒŒì¼ëª… ìƒì„± (ID + ì˜ì–´ ì´ë¦„)
            file_slug = slugify(ai_en_name)
            file_name = f"{cid}_{file_slug}.md"
            file_path = os.path.join(CONTENT_DIR, file_name)

            # ë©”íƒ€ë°ì´í„° êµ¬ì„± (ìƒì„¸í˜ì´ì§€ ìƒë‹¨ì— í‘œì‹œë  ì •ë³´ë“¤)
            metadata = {
                "id": cid,
                "title": str(row['name']),
                "title_en": ai_en_name,
                "address": str(row['location']),
                "subsidies": int(row['subsidy_count']),
                "category": "Japan SME",
                "contact": f"https://www.google.com/search?q={row['name']}+contact+website"
            }
            
            # YAML Frontmatterì™€ ë³¸ë¬¸ì„ í•©ì³ì„œ .md íŒŒì¼ ì €ì¥
            post = frontmatter.Post(ai_content, **metadata)
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(frontmatter.dumps(post))
            
            print(f"   âœ… ì™„ë£Œ: {file_name} ({len(ai_content)} chars)")
            count += 1
            time.sleep(5) # í• ë‹¹ëŸ‰ ë³´í˜¸
            
        except Exception as e:
            print(f"   âŒ ì—ëŸ¬: {e}")
            time.sleep(10)

    print(f"\nğŸ ì‘ì—… ì¢…ë£Œ.")

if __name__ == "__main__":
    generate_md()