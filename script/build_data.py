import sys
import os

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ìƒìœ„ í´ë”(í”„ë¡œì íŠ¸ ë£¨íŠ¸)ë¥¼ sys.pathì— ì¶”ê°€í•˜ì—¬ app íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ìˆê²Œ í•¨
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import frontmatter
import google.generativeai as genai
from dotenv import load_dotenv
import time
import re
import json
from datetime import datetime
import argparse

# app/config.py ì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
from app.config import (
    CSV_PATH, CONTENT_DIR, DATA_DIR, INDEX_PATH, 
    SITEMAP_PATH, DOMAIN, DAILY_LIMIT, CATEGORIES
)

# --- AI ì„¤ì • ---
load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
MODEL = genai.GenerativeModel('gemini-1.5-flash')

# --- í—¬í¼ í•¨ìˆ˜ ---
def slugify(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]+', '-', text)
    return text.strip('-')

# --- í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ë“¤ ---

def generate_new_content():
    """(Daily Task) CSVë¥¼ ì½ì–´ ìƒˆë¡œìš´ .md ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if not os.path.exists(CSV_PATH):
        print(f"âŒ ì›ë³¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {CSV_PATH}")
        return

    df = pd.read_csv(CSV_PATH)
    os.makedirs(CONTENT_DIR, exist_ok=True)
    
    count = 0
    print(f"ğŸš€ ì´ {len(df)}ê°œ ê¸°ì—… ì¤‘ ìµœëŒ€ {DAILY_LIMIT}ê°œì˜ ì‹ ê·œ ë¦¬í¬íŠ¸ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    for _, row in df.iterrows():
        if count >= DAILY_LIMIT:
            print(f" ì¼ì¼ ìƒì„± ì œí•œ({DAILY_LIMIT}ê°œ)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
            break
        
        cid = f"jp_{row['corporate_number']}"
        # ì´ë¯¸ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ê±´ë„ˆëœ€ (íŒŒì¼ëª…ì— cidê°€ í¬í•¨ëœ ê²½ìš°)
        if any(f.startswith(cid) for f in os.listdir(CONTENT_DIR)):
            continue

        print(f"   [ {count+1} / {DAILY_LIMIT} ] '{row['name']}' ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        prompt = f"""
        Act as a Senior Business Analyst. Analyze the following company and provide a B2B report.
        - Company: {row['name']}
        - Location: {row['location']}
        - Gov Info: {row['subsidy_titles'] if pd.notna(row['subsidy_titles']) else "Verified SME"}

        [Output Instructions]
        Line 1: A formal English name. If none is found, REPEAT the original Japanese name.
        Line 2: Choose the ONE most appropriate category from this list: {', '.join(CATEGORIES)}. Output in the format "Category: [Chosen Category]".
        Line 3: ---BODY---
        Line 4 and beyond: Full Detailed Markdown Report.

        [Analyst's Note Instructions]
        - At the VERY BEGINNING of the markdown body, create a short, insightful "Analyst's Note".
        - This note must be formatted as a Markdown blockquote.
        - It should summarize the company's core B2B value proposition in 2-3 sentences.

        [Content Focus & Formatting Rules]
        - Professional B2B perspective. Be verbose.
        - Use standard Markdown: headings (##), lists (*), bolding (**).
        """
        
        try:
            response = MODEL.generate_content(prompt)
            full_response = response.text.strip()

            if "---BODY---" in full_response:
                header_part, ai_content = full_response.split("---BODY---", 1)
            else:
                # êµ¬ë¶„ìê°€ ì—†ì„ ê²½ìš°ì˜ ì˜ˆì™¸ ì²˜ë¦¬
                lines = full_response.split('\n')
                header_part = lines[0]
                ai_content = "\n".join(lines[1:])

            header_lines = header_part.strip().split('\n')
            ai_en_name = header_lines[0].strip()
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            ai_category = "Services" # ê¸°ë³¸ê°’
            if len(header_lines) > 1 and "Category:" in header_lines[1]:
                ai_category = header_lines[1].replace("Category:", "").strip()
                if ai_category not in CATEGORIES:
                    ai_category = "Services"

            # ì˜ë¬¸ëª… ê²€ì¦ (ì˜ë¬¸ì´ ì•„ë‹ˆë©´ ì›ë³¸ ì¼ë³¸ì–´ ì´ë¦„ ì‚¬ìš©)
            if not ai_en_name.isascii():
                ai_en_name = str(row['name'])

            file_slug = slugify(ai_en_name)
            file_name = f"{cid}_{file_slug}.md" if file_slug else f"{cid}.md"
            file_path = os.path.join(CONTENT_DIR, file_name)

            metadata = {
                "id": cid,
                "title": str(row['name']),
                "title_en": ai_en_name,
                "address": str(row['location']),
                "subsidies": int(row['subsidy_count']),
                "category": ai_category,
                "contact": f"https://www.google.com/search?q={row['name']}+contact+website"
            }
            
            post = frontmatter.Post(ai_content.strip(), **metadata)
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(frontmatter.dumps(post))
            
            print(f"   âœ… ì™„ë£Œ: {file_name} (Category: {ai_category})")
            count += 1
            time.sleep(5) # API ì†ë„ ì œí•œ ê³ ë ¤
            
        except Exception as e:
            print(f"   âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            time.sleep(10)
    
    if count == 0:
        print("ìƒì„±í•  ìƒˆë¡œìš´ ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")


def migrate_missing_categories():
    """(Migration Task) ê¸°ì¡´ .md íŒŒì¼ì— ì¹´í…Œê³ ë¦¬ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì¶”ê°€í•©ë‹ˆë‹¤."""
    print("ğŸš€ ê¸°ì¡´ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì— ëŒ€í•œ ì¹´í…Œê³ ë¦¬ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    if not os.path.exists(CSV_PATH):
        print(f"âŒ ì›ë³¸ ë°ì´í„° íŒŒì¼({CSV_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    # CSV ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(CSV_PATH)
    df['id'] = 'jp_' + df['corporate_number'].astype(str)
    df.set_index('id', inplace=True)
    company_data_map = df.to_dict('index')

    files_to_update = [f for f in os.listdir(CONTENT_DIR) if f.endswith('.md')]
    total_files = len(files_to_update)
    updated_count = 0
    print(f"ì´ {total_files}ê°œì˜ íŒŒì¼ì„ ëŒ€ìƒìœ¼ë¡œ ê²€ì‚¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

    for i, filename in enumerate(files_to_update):
        file_path = os.path.join(CONTENT_DIR, filename)
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                post = frontmatter.load(f)
            
            # ì´ë¯¸ ìœ íš¨í•œ ì¹´í…Œê³ ë¦¬ê°€ ìˆìœ¼ë©´ íŒ¨ìŠ¤
            if 'category' in post.metadata and post.metadata['category'] in CATEGORIES:
                continue
            
            company_id = post.metadata.get('id')
            if not company_id or company_id not in company_data_map:
                print(f"   âš ï¸ [{i+1}/{total_files}] '{filename}' ì›ë³¸ ë°ì´í„° ëˆ„ë½. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            row = company_data_map[company_id]
            print(f"   - [{i+1}/{total_files}] '{filename}' ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì¤‘...")
            
            prompt = f"""
            Analyze the company info and choose ONE category from: {', '.join(CATEGORIES)}.
            - Company: {row['name']}
            - Info: {post.content[:500]}
            Output ONLY the chosen category name.
            """
            
            response = MODEL.generate_content(prompt)
            ai_category = response.text.strip()
            
            if ai_category not in CATEGORIES:
                print(f"     -> ê²½ê³ : AIê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ì¹´í…Œê³ ë¦¬('{ai_category}')ë¥¼ ë°˜í™˜í•˜ì—¬ 'Services'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                ai_category = "Services"
            
            post.metadata['category'] = ai_category
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(frontmatter.dumps(post))
                
            print(f"     -> âœ… ì™„ë£Œ. ì¹´í…Œê³ ë¦¬: {ai_category}")
            updated_count += 1
            time.sleep(2)
            
        except Exception as e:
            print(f"   âŒ [{i+1}/{total_files}] '{filename}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
            
    if updated_count == 0:
        print("ëª¨ë“  íŒŒì¼ì— ì´ë¯¸ ìœ íš¨í•œ ì¹´í…Œê³ ë¦¬ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.")
    else:
        print(f"\nâœ… ì´ {updated_count}ê°œì˜ íŒŒì¼ì— ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")


def update_index_and_sitemap():
    """(Final Task) ê²€ìƒ‰ ì¸ë±ìŠ¤ì™€ ì‚¬ì´íŠ¸ë§µì„ ìµœì‹  ìƒíƒœë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤."""
    if not os.path.exists(CONTENT_DIR):
        print(f"âš ï¸ ì½˜í…ì¸  í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {CONTENT_DIR}")
        return
    
    os.makedirs(DATA_DIR, exist_ok=True)
    # sitemapì´ ì €ì¥ë  static í´ë” í™•ë³´ (config.pyì˜ ê²½ë¡œ ì˜ì¡´)
    static_dir_path = os.path.dirname(SITEMAP_PATH)
    os.makedirs(static_dir_path, exist_ok=True)

    index_data = []
    sitemap_urls = []
    
    print(f"\nğŸ” '{CONTENT_DIR}' í´ë”ë¥¼ ìŠ¤ìº”í•˜ì—¬ ì¸ë±ì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    for filename in os.listdir(CONTENT_DIR):
        if filename.endswith('.md'):
            try:
                file_path = os.path.join(CONTENT_DIR, filename)
                with open(file_path, 'r', encoding='utf-8') as f:
                    post = frontmatter.load(f)
                
                file_slug = filename.replace(".md", "")
                
                # ê²€ìƒ‰ ì¸ë±ìŠ¤ ë°ì´í„°
                index_data.append({
                    "id": post.metadata.get('id', ''),
                    "file": file_slug,
                    "n": post.metadata.get('title', ''),
                    "en": post.metadata.get('title_en', ''),
                    "l": str(post.metadata.get('address', ''))[:30],
                    "s": post.metadata.get('subsidies', 0),
                    "c": post.metadata.get('category', 'Services')
                })

                # ì‚¬ì´íŠ¸ë§µ URL
                mtime = os.path.getmtime(file_path)
                lastmod = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d')
                
                sitemap_urls.append(f"""
    <url>
        <loc>{DOMAIN}/company/{file_slug}</loc>
        <lastmod>{lastmod}</lastmod>
        <changefreq>weekly</changefreq>
        <priority>0.8</priority>
    </url>""")
            except Exception as e:
                print(f"âš ï¸ {filename} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
    
    # JSON ì¸ë±ìŠ¤ ì €ì¥
    with open(INDEX_PATH, 'w', encoding='utf-8') as f:
        json.dump(index_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… ê²€ìƒ‰ ì¸ë±ìŠ¤ ê°±ì‹  ì™„ë£Œ ({len(index_data)}ê°œ ê¸°ì—…)")

    # Sitemap XML ì €ì¥
    sitemap_content = f"""<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
    <url>
        <loc>{DOMAIN}/</loc>
        <changefreq>daily</changefreq>
        <priority>1.0</priority>
    </url>
    {''.join(sitemap_urls)}
</urlset>"""
    
    with open(SITEMAP_PATH, 'w', encoding='utf-8') as f:
        f.write(sitemap_content)
    print(f"âœ… ì‚¬ì´íŠ¸ë§µ ìƒì„± ì™„ë£Œ ({len(sitemap_urls)}ê°œ ë§í¬) -> {SITEMAP_PATH}")


# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
if __name__ == "__main__":
    # ëª…ë ¹ì¤„ ì¸ìˆ˜ íŒŒì„œ ì„¤ì •
    parser = argparse.ArgumentParser(description="CompanyDB ì½˜í…ì¸  ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("command", choices=["daily", "migrate", "rebuild", "index"], help="ì‹¤í–‰í•  ì‘ì—…ì„ ì„ íƒí•©ë‹ˆë‹¤.")
    
    args = parser.parse_args()
    
    print("="*40)
    print(f"  CompanyDB ì½˜í…ì¸  ê´€ë¦¬: '{args.command}' ì‘ì—… ì‹œì‘")
    print("="*40)

    # ì„ íƒëœ ì‘ì—…ì— ë”°ë¼ í•¨ìˆ˜ ì‹¤í–‰
    if args.command == "daily":
        generate_new_content()
        update_index_and_sitemap()
    elif args.command == "migrate":
        migrate_missing_categories()
        update_index_and_sitemap()
    elif args.command == "rebuild":
        generate_new_content()
        migrate_missing_categories()
        update_index_and_sitemap()
    elif args.command == "index":
        update_index_and_sitemap()

    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("="*40)