import pandas as pd
import os
import frontmatter
import google.generativeai as genai
from dotenv import load_dotenv
import time
import re
import json
from datetime import datetime
import argparse # ëª…ë ¹ì¤„ ì¸ìˆ˜ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# --- ì„¤ì •: ìƒìˆ˜ ëª¨ìŒ ---
load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

MODEL = genai.GenerativeModel('gemini-1.5-flash')
DAILY_LIMIT = 10
CATEGORIES = ["Manufacturing", "Technology", "Electronics", "Medical", "Construction", "Services"]

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
CSV_PATH = os.path.join(BASE_DIR, 'data', 'Total_Premium_Japan_SMEs.csv')
CONTENT_DIR = os.path.join(BASE_DIR, 'app', 'content')
DATA_DIR = os.path.join(BASE_DIR, 'data')
INDEX_PATH = os.path.join(DATA_DIR, 'search_index.json')
SITEMAP_PATH = os.path.join(BASE_DIR, 'app', 'static', 'sitemap.xml')
DOMAIN = "https://companydb.net"


# --- í—¬í¼ í•¨ìˆ˜ ---
def slugify(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9]+', '-', text)
    return text.strip('-')

# --- í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ë“¤ ---

def generate_new_content():
    """(Daily Task) CSVë¥¼ ì½ì–´ ìƒˆë¡œìš´ .md ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    # ... (ê¸°ì¡´ run_daily_update.pyì˜ generate_new_content í•¨ìˆ˜ì™€ ë™ì¼) ...
    # (ì´í•˜ ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼í•˜ë¯€ë¡œ ìƒëµí•˜ì§€ ì•Šê³  ì „ì²´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤)
    if not os.path.exists(CSV_PATH):
        print(f"âŒ ì›ë³¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {CSV_PATH}")
        return

    df = pd.read_csv(CSV_PATH)
    os.makedirs(CONTENT_DIR, exist_ok=True)
    
    count = 0
    print(f"ğŸš€ ì´ {len(df)}ê°œ ê¸°ì—… ì¤‘ ìµœëŒ€ {DAILY_LIMIT}ê°œì˜ ì‹ ê·œ ë¦¬í¬íŠ¸ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    for _, row in df.iterrows():
        if count >= DAILY_LIMIT:
            print(f" ì¼ì¼ ìƒì„± ì œí•œ({DAILY_LIMIT}ê°œ)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
            break
        
        cid = f"jp_{row['corporate_number']}"
        if any(f.startswith(cid) for f in os.listdir(CONTENT_DIR)):
            continue

        print(f"   [ {count+1} / {DAILY_LIMIT} ] '{row['name']}' ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        prompt = f"""
        Act as a Senior Business Analyst. Analyze the following company and provide a B2B report.
        - Company: {row['name']}
        - Location: {row['location']}
        - Gov Info: {row['subsidy_titles'] if pd.notna(row['subsidy_titles']) else "Verified SME"}
        [Output Instructions]
        Line 1: A formal English name. If none is found, REPEAT the original Japanese name.
        Line 2: Choose the ONE most appropriate category from this list: {', '.join(CATEGORIES)}. Output in the format "Category: [Chosen Category]".
        Line 3: ---BODY---
        Line 4 and beyond: Full Detailed Markdown Report.
        [Analyst's Note Instructions]
        - At the VERY BEGINNING of the markdown body, create a short, insightful "Analyst's Note" blockquote.
        [Content Focus & Formatting Rules]
        - Professional B2B perspective. Use standard Markdown.
        """
        
        try:
            response = MODEL.generate_content(prompt)
            full_response = response.text.strip()
            header_part, ai_content = full_response.split("---BODY---", 1)
            header_lines = header_part.strip().split('\n')
            ai_en_name = header_lines[0].strip()
            ai_category = "Services"
            if len(header_lines) > 1 and "Category:" in header_lines[1]:
                ai_category = header_lines[1].replace("Category:", "").strip()
                if ai_category not in CATEGORIES: ai_category = "Services"
            if not ai_en_name.isascii(): ai_en_name = str(row['name'])
            file_slug = slugify(ai_en_name)
            file_name = f"{cid}_{file_slug}.md" if file_slug else f"{cid}.md"
            file_path = os.path.join(CONTENT_DIR, file_name)
            metadata = {
                "id": cid, "title": str(row['name']), "title_en": ai_en_name,
                "address": str(row['location']), "subsidies": int(row['subsidy_count']),
                "category": ai_category,
                "contact": f"https://www.google.com/search?q={row['name']}+contact+website"
            }
            post = frontmatter.Post(ai_content.strip(), **metadata)
            with open(file_path, "w", encoding="utf-8") as f: f.write(frontmatter.dumps(post))
            print(f"   âœ… ì™„ë£Œ: {file_name} (Category: {ai_category})")
            count += 1
            time.sleep(5)
        except Exception as e:
            print(f"   âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            time.sleep(10)
    if count == 0: print("ìƒì„±í•  ìƒˆë¡œìš´ ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")


def migrate_missing_categories():
    """(Migration Task) ê¸°ì¡´ .md íŒŒì¼ì— ì¹´í…Œê³ ë¦¬ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì¶”ê°€í•©ë‹ˆë‹¤."""
    # ... (ê¸°ì¡´ update_categories.pyì˜ add_category_to_existing_files í•¨ìˆ˜ì™€ ë™ì¼) ...
    print("ğŸš€ ê¸°ì¡´ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì— ëŒ€í•œ ì¹´í…Œê³ ë¦¬ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    if not os.path.exists(CSV_PATH):
        print(f"âŒ ì›ë³¸ ë°ì´í„° íŒŒì¼({CSV_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    df = pd.read_csv(CSV_PATH)
    df['id'] = 'jp_' + df['corporate_number'].astype(str)
    df.set_index('id', inplace=True)
    company_data_map = df.to_dict('index')

    files_to_update = [f for f in os.listdir(CONTENT_DIR) if f.endswith('.md')]
    total_files = len(files_to_update)
    updated_count = 0
    print(f"ì´ {total_files}ê°œì˜ íŒŒì¼ì„ ëŒ€ìƒìœ¼ë¡œ ê²€ì‚¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

    for i, filename in enumerate(files_to_update):
        file_path = os.path.join(CONTENT_DIR, filename)
        try:
            with open(file_path, 'r', encoding='utf-8') as f: post = frontmatter.load(f)
            if 'category' in post.metadata and post.metadata['category'] in CATEGORIES: continue
            
            company_id = post.metadata.get('id')
            if not company_id or company_id not in company_data_map:
                print(f"   âš ï¸ [{i+1}/{total_files}] '{filename}' ì›ë³¸ ë°ì´í„° ëˆ„ë½. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            row = company_data_map[company_id]
            print(f"   - [{i+1}/{total_files}] '{filename}' ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì¤‘...")
            prompt = f"""Analyze the company and choose ONE category from {', '.join(CATEGORIES)}.
            - Company: {row['name']}
            - Info: {post.content[:500]}
            Output ONLY the chosen category name."""
            response = MODEL.generate_content(prompt)
            ai_category = response.text.strip()
            if ai_category not in CATEGORIES:
                print(f"     -> ê²½ê³ : AIê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ì¹´í…Œê³ ë¦¬('{ai_category}')ë¥¼ ë°˜í™˜í•˜ì—¬ 'Services'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                ai_category = "Services"
            post.metadata['category'] = ai_category
            with open(file_path, 'w', encoding='utf-8') as f: f.write(frontmatter.dumps(post))
            print(f"     -> âœ… ì™„ë£Œ. ì¹´í…Œê³ ë¦¬: {ai_category}")
            updated_count += 1
            time.sleep(2)
        except Exception as e:
            print(f"   âŒ [{i+1}/{total_files}] '{filename}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
    if updated_count == 0: print("ëª¨ë“  íŒŒì¼ì— ì´ë¯¸ ìœ íš¨í•œ ì¹´í…Œê³ ë¦¬ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.")
    else: print(f"\nâœ… ì´ {updated_count}ê°œì˜ íŒŒì¼ì— ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")


def update_index_and_sitemap():
    """(Final Task) ê²€ìƒ‰ ì¸ë±ìŠ¤ì™€ ì‚¬ì´íŠ¸ë§µì„ ìµœì‹  ìƒíƒœë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤."""
    # ... (ê¸°ì¡´ run_daily_update.pyì˜ update_index_and_sitemap í•¨ìˆ˜ì™€ ë™ì¼) ...
    if not os.path.exists(CONTENT_DIR):
        print(f"âš ï¸ ì½˜í…ì¸  í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {CONTENT_DIR}")
        return
    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, 'app', 'static'), exist_ok=True)
    index_data, sitemap_urls = [], []
    print(f"\nğŸ” '{CONTENT_DIR}' í´ë”ë¥¼ ìŠ¤ìº”í•˜ì—¬ ì¸ë±ì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    for filename in os.listdir(CONTENT_DIR):
        if filename.endswith('.md'):
            try:
                file_path = os.path.join(CONTENT_DIR, filename)
                with open(file_path, 'r', encoding='utf-8') as f: post = frontmatter.load(f)
                file_slug = filename.replace(".md", "")
                index_data.append({
                    "id": post.metadata.get('id', ''), "file": file_slug,
                    "n": post.metadata.get('title', ''), "en": post.metadata.get('title_en', ''),
                    "l": str(post.metadata.get('address', ''))[:30], "s": post.metadata.get('subsidies', 0),
                    "c": post.metadata.get('category', 'Services')
                })
                mtime = os.path.getmtime(file_path)
                lastmod = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d')
                sitemap_urls.append(f'\n    <url>\n        <loc>{DOMAIN}/company/{file_slug}</loc>\n        <lastmod>{lastmod}</lastmod>\n        <changefreq>weekly</changefreq>\n        <priority>0.8</priority>\n    </url>')
            except Exception as e:
                print(f"âš ï¸ {filename} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
    with open(INDEX_PATH, 'w', encoding='utf-8') as f: json.dump(index_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… ê²€ìƒ‰ ì¸ë±ìŠ¤ ê°±ì‹  ì™„ë£Œ ({len(index_data)}ê°œ ê¸°ì—…)")
    sitemap_content = f'<?xml version="1.0" encoding="UTF-8"?>\n<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n    <url>\n        <loc>{DOMAIN}/</loc>\n        <changefreq>daily</changefreq>\n        <priority>1.0</priority>\n    </url>{"".join(sitemap_urls)}\n</urlset>'
    with open(SITEMAP_PATH, 'w', encoding='utf-8') as f: f.write(sitemap_content)
    print(f"âœ… ì‚¬ì´íŠ¸ë§µ ìƒì„± ì™„ë£Œ ({len(sitemap_urls)}ê°œ ë§í¬)")

# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
if __name__ == "__main__":
    # ëª…ë ¹ì¤„ ì¸ìˆ˜ íŒŒì„œ ì„¤ì •
    parser = argparse.ArgumentParser(description="CompanyDB ì½˜í…ì¸  ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("command", choices=["daily", "migrate", "rebuild", "index"], help="ì‹¤í–‰í•  ì‘ì—…ì„ ì„ íƒí•©ë‹ˆë‹¤.")
    
    args = parser.parse_args()
    
    print("="*40)
    print(f"  CompanyDB ì½˜í…ì¸  ê´€ë¦¬: '{args.command}' ì‘ì—… ì‹œì‘")
    print("="*40)

    # ì„ íƒëœ ì‘ì—…ì— ë”°ë¼ í•¨ìˆ˜ ì‹¤í–‰
    if args.command == "daily":
        generate_new_content()
        update_index_and_sitemap()
    elif args.command == "migrate":
        migrate_missing_categories()
        update_index_and_sitemap()
    elif args.command == "rebuild":
        generate_new_content()
        migrate_missing_categories()
        update_index_and_sitemap()
    elif args.command == "index":
        update_index_and_sitemap()

    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("="*40)